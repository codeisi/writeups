{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_7kkHZ4K_gI"
   },
   "source": [
    "#### Pacific Warriors Meetup 15-Jan-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fKQkfp9Lp7Z"
   },
   "source": [
    "# **Convolutional Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6oP1KDXrv7jW"
   },
   "source": [
    "## What are CNNs?\n",
    "\n",
    "A very important Deep Learning technique used in Image Classification, Object Detection, Machine Vision <br> \n",
    "<br>\n",
    "Inspired by research done on the Visual Cortex of mammals ([Hubel and Wiesel Cat experiment 1959](https://www.youtube.com/watch?v=IOHayh06LJ4))  <br> \n",
    "<br>\n",
    "Visual Cortex consists of a layered architecture. Each layer consists of groups of neurons designed specifically to recognize different shapes. <br> \n",
    "<br>\n",
    "<img center src='images/cnn-human-vision-analogy.jpeg?raw=1' height=50% width=75% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNP10--7SJYr"
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ysr65FWBrZfR"
   },
   "source": [
    "**Typical Layers**\n",
    "\n",
    "<br>\n",
    "Convolutional Layer + Activation Function\n",
    "\n",
    "Pooling Layer\n",
    "\n",
    "Fully Connected Layer\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='images/cnn-arch-2.png?raw=1' height=50% width=75%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TK2wtmwxsG9d"
   },
   "source": [
    "# Convolutional Layer + Activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2PLnkMRIsUWS"
   },
   "source": [
    "##Goal \n",
    "Feature Extraction, Find Spatial Features <br>\n",
    "\n",
    "Apply series of image filters aka '**Convolutional Kernels**' to input image.<br> \n",
    "Filters trained to recognize low-level features in an image.<br> Activations of these filters across the image returns a '**Feature Map**' for each filter. <br> <br>\n",
    "\n",
    "\n",
    "**Activation Function** <br>\n",
    "Normalizes pixel values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aCwRIL78scG7"
   },
   "source": [
    "## Convolution Operation\n",
    "\n",
    "\n",
    "\n",
    "<img center src='images/convolution-working2.png?raw=1' height=50% width=75% align=\"center\"/>\n",
    "\n",
    "(`88*1 + 126*0 + 145*1`) + (`86*1 + 125*1 + 142*0`) + (`85*0 + 124*0 + 141*0`) <br>\n",
    "= (88 + 145) + (86 + 125 ) <br>\n",
    "= 233 + 211 <br>\n",
    "= 444\n",
    "\n",
    "Size of the feature map\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvwMVNKCstns"
   },
   "source": [
    "## Convolutional Layer - Result <br>\n",
    "\n",
    "<img center src='images/stride-convolution.gif?raw=1' align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f11tWx_ltgvw"
   },
   "source": [
    "# Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxLb1I1utj1T"
   },
   "source": [
    "## Goal\n",
    "\n",
    "Dimensionality Reduction\n",
    "\n",
    "Reduces the x-y size of of the extracted feature maps and only keeps the most *active* pixel values.<br>\n",
    "Preserves Spatial Information. <br>\n",
    "<br>\n",
    "Few Types of Pooling <br>\n",
    "\n",
    "*   Max Pooling (most common)\n",
    "*   Average Pooling\n",
    "*   Sum Pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6bq54ZytqZ9"
   },
   "source": [
    "### Max Pooling Example <br> \n",
    "2x2 pooling kernel, with a stride of 2. Only the maximum pixel values in 2x2 remain in the new, pooled output.\n",
    "\n",
    "\n",
    "<img src='images/maxpooling_ex.png?raw=1' height=50% width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S69hM-ejuHxr"
   },
   "source": [
    "# Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9wOMYGgNuKAc"
   },
   "source": [
    "## Goal\n",
    "\n",
    "Produce list of class scores & prediction. \n",
    "\n",
    "Last Fully Connected layer will have as many nodes as there are classes.\n",
    "\n",
    "<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/fully-connected-layers.gif?raw=1' height=50% width=75% align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDF0gHJuuKRk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGFdq8NevWnv"
   },
   "source": [
    "# Stride & Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJjO8TMZvatD"
   },
   "source": [
    "## Stride\n",
    "\n",
    "Number of pixels a filter moves at a time\n",
    "\n",
    "<img center src='images/full-padding-no-strides-transposed.gif?raw=1' height=25% width=25%  align=\"center\"/>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Padding\n",
    "\n",
    "Addition of pixels to the edge of the image. <br>\n",
    "Preserves border information.<br>\n",
    "\n",
    "<img center src='images/padding-example.png?raw=1' height=25% width=50%  align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Padding Modes\n",
    "\n",
    "Zeros (default): Add 1 or more layers of zeros (black) to the edges.<br>\n",
    "Reflection ([Interesting discussion here](https://twitter.com/karpathy/status/720622989289644033)) : Pads with reflection of outer edges of the input image or filter tensor. <br>\n",
    "Other options: Replicate, Circular \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3teuEBYLNFd"
   },
   "source": [
    "# Number of Parameters in a Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNMRQoCgSCgD"
   },
   "source": [
    "Number of parameters in the convolutional layer = \n",
    "K&#42;F&#42;F&#42;D_in + K. <br>\n",
    "\n",
    "K - Number of filters <br>\n",
    "F - Filter/Kernel size <br>\n",
    "D_in - Depth of previous layer typically 1 or 3 (RGB and grayscale, respectively). <br>\n",
    "<br>\n",
    "As Kernel size increases, <br> \n",
    "* Number of parameters increase & <br>\n",
    "* Size of patterns detected increases <br>\n",
    "\n",
    "As number of filters increase, number of parameters increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmeqM3FtI0gd"
   },
   "source": [
    "# Shape of a Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCMSHcfRI9Z3"
   },
   "source": [
    "The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. \n",
    "\n",
    "The spatial dimensions of a convolutional layer can be calculated as: `(W_inâˆ’F+2P)/S+1`\n",
    "\n",
    "K - the number of filters <br>\n",
    "F - filter/kernel size <br>\n",
    "S - stride <br>\n",
    "P - padding <br>\n",
    "W_in - input size (width/height (square) of the previous layer) <br>\n",
    "\n",
    "The depth of the convolutional layer will always equal the number of filters K.\n",
    "<br><br>\n",
    "\n",
    "### Sample Calculation (Quiz 6.35)\n",
    "For an input image that is 130x130 (x, y) and 3 in depth (RGB). \n",
    "Say, this image goes through the following layers in order: \n",
    "<br> <br>\n",
    "nn.Conv2d(3, 10, 3) `stride=1, padding=0 output_size=(130-3+(2x0))/1+1=128` `depth=10`\n",
    "<br>\n",
    "nn.MaxPool2d(4, 4) `output_size=128/4=32` <br><br>\n",
    "nn.Conv2d(10, 20, 5, padding=2) `stride=1 output_size=(32-5+(2x2))/1+1=32` <br>\n",
    "nn.MaxPool2d(2, 2) `output_size=32/2=16` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyfaIpRzjUr9"
   },
   "source": [
    "# Kernel Size Considerations\n",
    "\n",
    "<table style=\"width:80%\" align=center>\n",
    "  <tr>\n",
    "    <th width=50% align=left>Smaller Filter Sizes</th>\n",
    "    <th width=50% align=left>Larger Filter Sizes</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>It has a smaller receptive field as it looks at very few pixels at once.</td>\n",
    "    <td>Larger receptive field per layer.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Highly local features extracted without much image overview.</td>\n",
    "    <td>Quite generic features extracted spread across the image.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Therefore captures smaller, complex features in the image.</td>\n",
    "    <td>Therefore captures the basic components in the image.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Amount of information extracted will be vast, maybe useful in later layers.</td>\n",
    "    <td>Amount of information extracted are considerably lesser.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Slow reduction in the image dimension can make the network deep</td>\n",
    "    <td>Fast reduction in the image dimension makes the network shallow</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Better weight sharing</td>\n",
    "    <td>Poorer weight sharing</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>In an extreme scenario, using a 1x1 convolution is like treating each pixel as a useful feature.</td>\n",
    "    <td>Using a image sized filter is equivalent to a fully connected layer.</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Understanding Convolutional Neural Networks](https://towardsdatascience.com/understanding-convolutional-neural-networks-221930904a8e)\n",
    "\n",
    "[Deciding optimimal kernel size for CNN](https://towardsdatascience.com/deciding-optimal-filter-size-for-cnns-d6f7b56f9363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "name": "Presentation-CNNs.ipynb",
   "provenance": [
    {
     "file_id": "1f-5Ck68-ricb2KUOt79U78APAEEkiOEu",
     "timestamp": 1578866191766
    },
    {
     "file_id": "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/conv-visualization/maxpooling_visualization.ipynb",
     "timestamp": 1578537338809
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
