{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"Presentation-CNNs.ipynb","provenance":[{"file_id":"1f-5Ck68-ricb2KUOt79U78APAEEkiOEu","timestamp":1578866191766},{"file_id":"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/conv-visualization/maxpooling_visualization.ipynb","timestamp":1578537338809}]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"n_7kkHZ4K_gI","colab_type":"text"},"source":["#### Pacific Warriors Meetup 15-Jan-2020"]},{"cell_type":"markdown","metadata":{"id":"2fKQkfp9Lp7Z","colab_type":"text"},"source":["# **Convolutional Neural Networks**"]},{"cell_type":"markdown","metadata":{"id":"6oP1KDXrv7jW","colab_type":"text"},"source":["## What are CNNs?\n","\n","A very important Deep Learning technique used in Image Classification, Object Detection, Machine Vision <br> \n","<br>\n","Inspired by research done on the Visual Cortex of mammals ([Hubel and Wiesel Cat experiment 1959](https://www.youtube.com/watch?v=IOHayh06LJ4))  <br> \n","<br>\n","Visual Cortex consists of a layered architecture. Each layer consists of groups of neurons designed specifically to recognize different shapes. <br> \n","<br>\n","<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/cnn-human-vision-analogy.jpeg?raw=1' height=50% width=75% align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"HNP10--7SJYr","colab_type":"text"},"source":["# Architecture"]},{"cell_type":"markdown","metadata":{"id":"Ysr65FWBrZfR","colab_type":"text"},"source":["**Typical Layers**\n","\n","<br>\n","Convolutional Layer + Activation Function\n","\n","Pooling Layer\n","\n","Fully Connected Layer\n","\n","<br>\n","\n","<img src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/cnn-arch-2.png?raw=1' height=50% width=75%/>"]},{"cell_type":"markdown","metadata":{"id":"TK2wtmwxsG9d","colab_type":"text"},"source":["# Convolutional Layer + Activation function "]},{"cell_type":"markdown","metadata":{"id":"2PLnkMRIsUWS","colab_type":"text"},"source":["##Goal \n","Feature Extraction, Find Spatial Features <br>\n","\n","Apply series of image filters aka '**Convolutional Kernels**' to input image.<br> \n","Filters trained to recognize low-level features in an image.<br> Activations of these filters across the image returns a '**Feature Map**' for each filter. <br> <br>\n","\n","\n","**Activation Function** <br>\n","Normalizes pixel values.\n"]},{"cell_type":"markdown","metadata":{"id":"aCwRIL78scG7","colab_type":"text"},"source":["## Convolution Operation\n","\n","\n","\n","<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/convolution-working2.png?raw=1' height=50% width=75% align=\"center\"/>\n","\n","(`88*1 + 126*0 + 145*1`) + (`86*1 + 125*1 + 142*0`) + (`85*0 + 124*0 + 141*0`) <br>\n","= (88 + 145) + (86 + 125 ) <br>\n","= 233 + 211 <br>\n","= 444\n","\n","Size of the feature map\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YvwMVNKCstns","colab_type":"text"},"source":["## Convolutional Layer - Result <br>\n","\n","<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/stride-convolution.gif?raw=1' align=\"center\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f11tWx_ltgvw","colab_type":"text"},"source":["# Pooling Layer"]},{"cell_type":"markdown","metadata":{"id":"PxLb1I1utj1T","colab_type":"text"},"source":["## Goal\n","\n","Dimensionality Reduction\n","\n","Reduces the x-y size of of the extracted feature maps and only keeps the most *active* pixel values.<br>\n","Preserves Spatial Information. <br>\n","<br>\n","Few Types of Pooling <br>\n","\n","*   Max Pooling (most common)\n","*   Average Pooling\n","*   Sum Pooling\n"]},{"cell_type":"markdown","metadata":{"id":"R6bq54ZytqZ9","colab_type":"text"},"source":["### Max Pooling Example <br> \n","2x2 pooling kernel, with a stride of 2. Only the maximum pixel values in 2x2 remain in the new, pooled output.\n","\n","\n","<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/convolutional-neural-networks/conv-visualization/notebook_ims/maxpooling_ex.png?raw=1' height=50% width=50% />"]},{"cell_type":"markdown","metadata":{"id":"S69hM-ejuHxr","colab_type":"text"},"source":["# Fully Connected Layer"]},{"cell_type":"markdown","metadata":{"id":"9wOMYGgNuKAc","colab_type":"text"},"source":["## Goal\n","\n","Produce list of class scores & prediction. \n","\n","Last Fully Connected layer will have as many nodes as there are classes.\n","\n","<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/fully-connected-layers.gif?raw=1' height=50% width=75% align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"id":"MDF0gHJuuKRk","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"NGFdq8NevWnv","colab_type":"text"},"source":["# Stride & Padding"]},{"cell_type":"markdown","metadata":{"id":"XJjO8TMZvatD","colab_type":"text"},"source":["## Stride\n","\n","Number of pixels a filter moves at a time\n","\n","<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/full-padding-no-strides-transposed.gif?raw=1' height=25% width=25%  align=\"center\"/>\n","<br>\n","<br>\n","\n","## Padding\n","\n","Addition of pixels to the edge of the image. <br>\n","Preserves border information.<br>\n","\n","<img center src='https://github.com/codeisi/writeups/blob/master/deck-cnn/images/padding-example.png?raw=1' height=25% width=50%  align=\"center\"/>\n","\n","<br>\n","<br>\n","\n","## Padding Modes\n","\n","Zeros (default): Add 1 or more layers of zeros (black) to the edges.<br>\n","Reflection ([Interesting discussion here](https://twitter.com/karpathy/status/720622989289644033)) : Pads with reflection of outer edges of the input image or filter tensor. <br>\n","Other options: Replicate, Circular \n"]},{"cell_type":"markdown","metadata":{"id":"q3teuEBYLNFd","colab_type":"text"},"source":["# Number of Parameters in a Convolutional Layer"]},{"cell_type":"markdown","metadata":{"id":"BNMRQoCgSCgD","colab_type":"text"},"source":["Number of parameters in the convolutional layer = \n","K&#42;F&#42;F&#42;D_in + K. <br>\n","\n","K - Number of filters <br>\n","F - Filter/Kernel size <br>\n","D_in - Depth of previous layer typically 1 or 3 (RGB and grayscale, respectively). <br>\n","<br>\n","As Kernel size increases, <br> \n","* Number of parameters increase & <br>\n","* Size of patterns detected increases <br>\n","\n","As number of filters increase, number of parameters increase\n"]},{"cell_type":"markdown","metadata":{"id":"RmeqM3FtI0gd","colab_type":"text"},"source":["# Shape of a Convolutional Layer"]},{"cell_type":"markdown","metadata":{"id":"LCMSHcfRI9Z3","colab_type":"text"},"source":["The shape of a convolutional layer depends on the supplied values of kernel_size, input_shape, padding, and stride. \n","\n","The spatial dimensions of a convolutional layer can be calculated as: `(W_inâˆ’F+2P)/S+1`\n","\n","K - the number of filters <br>\n","F - filter/kernel size <br>\n","S - stride <br>\n","P - padding <br>\n","W_in - input size (width/height (square) of the previous layer) <br>\n","\n","The depth of the convolutional layer will always equal the number of filters K.\n","<br><br>\n","\n","### Sample Calculation (Quiz 6.35)\n","For an input image that is 130x130 (x, y) and 3 in depth (RGB). \n","Say, this image goes through the following layers in order: \n","<br> <br>\n","nn.Conv2d(3, 10, 3) `stride=1, padding=0 output_size=(130-3+(2x0))/1+1=128` `depth=10`\n","<br>\n","nn.MaxPool2d(4, 4) `output_size=128/4=32` <br><br>\n","nn.Conv2d(10, 20, 5, padding=2) `stride=1 output_size=(32-5+(2x2))/1+1=32` <br>\n","nn.MaxPool2d(2, 2) `output_size=32/2=16` <br>"]},{"cell_type":"markdown","metadata":{"id":"ZyfaIpRzjUr9","colab_type":"text"},"source":["# Kernel Size Considerations\n","\n","<table style=\"width:80%\" align=center>\n","  <tr>\n","    <th width=50% align=left>Smaller Filter Sizes</th>\n","    <th width=50% align=left>Larger Filter Sizes</th>\n","  </tr>\n","  <tr>\n","    <td>It has a smaller receptive field as it looks at very few pixels at once.</td>\n","    <td>Larger receptive field per layer.</td>\n","  </tr>\n","  <tr>\n","    <td>Highly local features extracted without much image overview.</td>\n","    <td>Quite generic features extracted spread across the image.</td>\n","  </tr>\n","  <tr>\n","    <td>Therefore captures smaller, complex features in the image.</td>\n","    <td>Therefore captures the basic components in the image.</td>\n","  </tr>\n","  <tr>\n","    <td>Amount of information extracted will be vast, maybe useful in later layers.</td>\n","    <td>Amount of information extracted are considerably lesser.</td>\n","  </tr>\n","  <tr>\n","    <td>Slow reduction in the image dimension can make the network deep</td>\n","    <td>Fast reduction in the image dimension makes the network shallow</td>\n","  </tr>\n","  <tr>\n","    <td>Better weight sharing</td>\n","    <td>Poorer weight sharing</td>\n","  </tr>\n","  <tr>\n","    <td>In an extreme scenario, using a 1x1 convolution is like treating each pixel as a useful feature.</td>\n","    <td>Using a image sized filter is equivalent to a fully connected layer.</td>\n","  </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"qvLYB04tijNc","colab_type":"text"},"source":["# Code Walkthru"]}]}